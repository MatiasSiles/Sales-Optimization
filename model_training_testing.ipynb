{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8PkTaa1qYE1tZ1b5VvprY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasSiles/Sales-Optimization/blob/main/model_training_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from typing import Dict, List, Tuple, Any, Optional, Union\n",
        "from datetime import datetime, timedelta\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV,\n",
        "    StratifiedKFold, KFold, TimeSeriesSplit\n",
        ")\n",
        "\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder,\n",
        "    OneHotEncoder, PolynomialFeatures\n",
        ")\n",
        "\n",
        "from sklearn.feature_selection import (\n",
        "    SelectKBest, f_regression, f_classif, mutual_info_regression,\n",
        "    mutual_info_classif, RFE, RFECV\n",
        ")\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor,\n",
        "    GradientBoostingClassifier, AdaBoostRegressor, AdaBoostClassifier,\n",
        "    ExtraTreesRegressor, ExtraTreesClassifier\n",
        ")\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet,\n",
        "    BayesianRidge, HuberRegressor\n",
        ")\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.decomposition import PCA, FactorAnalysis, TruncatedSVD\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix,\n",
        "    classification_report, silhouette_score\n",
        ")\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential, Model\n",
        "    from tensorflow.keras.layers import (\n",
        "        Dense, Dropout, BatchNormalization, Input, LSTM, GRU,\n",
        "        Conv1D, MaxPooling1D, Flatten, Embedding\n",
        "    )\n",
        "    from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "    from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    print(\"TensorFlow not available\")\n",
        "\n",
        "# Advanced ML Libraries\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "\n",
        "# Statistical Libraries\n",
        "from scipy import stats\n",
        "from scipy.stats import (\n",
        "    pearsonr, spearmanr, chi2_contingency, ttest_ind, mannwhitneyu,\n",
        "    kruskal, shapiro, jarque_bera, anderson\n",
        ")\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.stats.diagnostic import het_white, het_breuschpagan\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "\n",
        "from utils import (\n",
        "    DataPreprocessor, StatisticalAnalyzer, MachineLearningOptimizer\n",
        ")\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "kDKkWlyCt8Ah"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ],
      "metadata": {
        "id": "Y2fDj9TPR3fh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA for training models"
      ],
      "metadata": {
        "id": "k1JDmFar4-i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data upload\n",
        "df = pd.read_csv('/content/Sales_Business_Testing.csv')\n",
        "df.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "7ARtSl8T5Kyw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic info of the dataset\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\") # converts actual memory usage from bytes to megabytes\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "print(\"\\nMissing values:\")\n",
        "missing_values = df.isna().sum()\n",
        "print(missing_values[missing_values > 0])"
      ],
      "metadata": {
        "id": "5Czm-eeS47FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data analysis\n",
        "analyzer = StatisticalAnalyzer()\n",
        "descriptive_stats = analyzer.descriptive_statistics(df)\n",
        "\n",
        "print(descriptive_stats['numeric_summary'].round(2))\n",
        "\n",
        "# CATEGORICAL VARIABLES SUMMARY, in this case, there isn't categorical variables, because this testing datast don't have\n",
        "# for col, stats in descriptive_stats['categorical_summary'].items():\n",
        "#     print(f\"\\n{col}:\")\n",
        "#     print(f\"  Unique values: {stats['unique_values']}\")\n",
        "#     print(f\"  Mode: {stats['mode']}\")\n",
        "#     print(\"  Top 5 values:\")\n",
        "#     print(stats['value_counts'].head())\n",
        "\n",
        "# Distribution analysis\n",
        "print()\n",
        "descriptive_stats['skewness'].plot(kind='hist', label='skewness')\n",
        "descriptive_stats['kurtosis'].plot(kind='hist', label='kurtosis')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_VHZEnrP6WJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why i choose these targets?:\n",
        "\n",
        "*   total_amount: The aim is to predict the business's sales taking into account the variables that influence them, such as quantity sold, discounts prices, dates, etc\n",
        "*   profit: anticipate profits, to know which products are profitable, improve decision-making and optimize prices.\n",
        "*   satisfaction_score: Reduce churn, recommend products to each type\n",
        "ofcustomer, improve customer experience\n",
        "*   is_returned: reduce returns, optimize inventory, improve quality and products\n",
        "*   order_frequency: customer retention, detect loyal customers, personalize marketing, improve cash flow\n",
        "*   customer_lifetime_days: Seeking to ensure that customers remain active in the business for longer periods of time"
      ],
      "metadata": {
        "id": "OhRk5xVOSziz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = analyzer.correlation_analysis(df, method='pearson')\n",
        "\n",
        "# Find highly correlated features, this algorithm goes through the entire dataset by rows, comparing it with the threshold\n",
        "def find_high_correlations(corr_matrix, threshold=0.8):\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)): # 91 columns\n",
        "        for j in range(i+1, len(corr_matrix.columns)): # It is a loop where each iteration goes through all the columns while subtracting 1 column until reaching 0 columns\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                high_corr_pairs.append({\n",
        "                    'Feature1': corr_matrix.columns[i], # add column 1 which participates in high correlations\n",
        "                    'Feature2': corr_matrix.columns[j], # add column 2 which participates in high correlations\n",
        "                    'Correlation': corr_matrix.iloc[i, j] # add the value\n",
        "                })\n",
        "    return pd.DataFrame(high_corr_pairs)\n",
        "\n",
        "high_corr = find_high_correlations(corr_matrix, threshold=0.7)\n",
        "\n",
        "if not high_corr.empty: # check whether or not high correlations were found\n",
        "    print(f\"\\nHigh correlations found (>0.7):\")\n",
        "    print(high_corr)\n",
        "else:\n",
        "    print(\"\\nNo high correlations found (>0.7)\")\n",
        "\n",
        "# Visualize correlation matrix, It's a confusing graph but I modified it to only show the correlations.\n",
        "plt.figure(figsize=(20, 16))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) # only shows half of the heat map\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Target variable correlations\n",
        "TARGET_VARIABLES = ['total_amount', 'profit', 'satisfaction_score','is_returned', 'order_frequency', 'customer_lifetime_days']\n",
        "\n",
        "for target in TARGET_VARIABLES:\n",
        "    if target in df.columns:\n",
        "        target_corr = corr_matrix[target].abs().sort_values(ascending=False) # find the 10 best correlations for each target\n",
        "        print(f\"\\n{target} - Top 10 correlations:\")\n",
        "        print(target_corr.head(10))"
      ],
      "metadata": {
        "id": "OmoOq4VbGfa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i decided to do this because i think that can exist hidden relationships in important features for the training mode. If exist that relationship, probably can improve the model's performance. Whether a difference or relationship in the data is real or just a matter of luck\n",
        "\n",
        "Significance:\n",
        "*   p < 0.05 ‚Üí is significant: probably is real the relationship\n",
        "*   p > 0.05 ‚Üí is not significant"
      ],
      "metadata": {
        "id": "TiIH6ufdmoU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It looks for relationships between numerical and categorical variables\n",
        "categorical_cols = ['customer_gender', 'customer_segment', 'sales_channel', 'payment_method']\n",
        "numerical_targets = ['total_amount', 'profit', 'satisfaction_score']\n",
        "\n",
        "hypothesis_results = {}\n",
        "\n",
        "for cat_col in categorical_cols:\n",
        "    if cat_col in df.columns:\n",
        "        for num_col in numerical_targets:\n",
        "            if num_col in df.columns:\n",
        "                print(f\"\\nTesting {cat_col} vs {num_col}:\")\n",
        "\n",
        "                test_result = analyzer.hypothesis_testing(\n",
        "                    df, cat_col, num_col, test_type='auto'\n",
        "                )\n",
        "\n",
        "                hypothesis_results[f\"{cat_col}_vs_{num_col}\"] = test_result\n",
        "\n",
        "                print(f\"  Test used: {test_result['test_used']}\")\n",
        "                print(f\"  P-value: {test_result['p_value']:.6f}\")\n",
        "                print(f\"  Significant: {'Yes' if test_result['significant'] else 'No'}\")"
      ],
      "metadata": {
        "id": "9IU0usRsO9nz",
        "outputId": "041af7b5-7c78-43b9-fb42-f4b9db969682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing customer_gender vs total_amount:\n",
            "  Test used: Kruskal-Wallis\n",
            "  P-value: 0.495825\n",
            "  Significant: ‚ùå No\n",
            "\n",
            "Testing customer_gender vs profit:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-1165455389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTesting {cat_col} vs {num_col}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 test_result = analyzer.hypothesis_testing(\n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 )\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mhypothesis_testing\u001b[0;34m(self, df, group_col, target_col, test_type)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# Multiple groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mgroup_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkruskal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgroup_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mtest_used\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Kruskal-Wallis'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# Multiple groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mgroup_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkruskal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgroup_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mtest_used\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Kruskal-Wallis'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4091\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4093\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4095\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4154\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4151\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \"\"\"\n\u001b[0;32m-> 4153\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4154\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4131\u001b[0m             )\n\u001b[1;32m   4132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4133\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   4134\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4135\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    685\u001b[0m             )\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             new_blocks = [\n\u001b[0m\u001b[1;32m    688\u001b[0m                 blk.take_nd(\n\u001b[1;32m    689\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             new_blocks = [\n\u001b[0;32m--> 688\u001b[0;31m                 blk.take_nd(\n\u001b[0m\u001b[1;32m    689\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;31m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m         new_values = algos.take_nd(\n\u001b[0m\u001b[1;32m   1308\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_value_for_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_np_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "time series analysis"
      ],
      "metadata": {
        "id": "Bmgm1SQEmcKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a date column for time series analysis\n",
        "if 'year' in df.columns and 'month' in df.columns:\n",
        "    df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
        "\n",
        "    # Perform time series analysis on total_amount\n",
        "    if 'total_amount' in df.columns:\n",
        "        print(\"üîÑ Analyzing sales over time...\")\n",
        "\n",
        "        ts_results = analyzer.time_series_analysis(\n",
        "            df, 'date', 'total_amount', freq='M'\n",
        "        )\n",
        "\n",
        "        print(f\"üìä Time series statistics:\")\n",
        "        print(f\"  Mean: ${ts_results['mean']:.2f}\")\n",
        "        print(f\"  Std: ${ts_results['std']:.2f}\")\n",
        "        if ts_results['autocorrelation']:\n",
        "            print(f\"  Autocorrelation (lag=1): {ts_results['autocorrelation']:.3f}\")\n",
        "\n",
        "        # Plot time series\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        ts_results['original_series'].plot(title='Total Sales Over Time',\n",
        "                                         color='blue', linewidth=2)\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Total Amount ($)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "        # Plot decomposition if available\n",
        "        if 'decomposition' in ts_results:\n",
        "            fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
        "            ts_results['original_series'].plot(ax=axes[0], title='Original')\n",
        "            ts_results['trend'].plot(ax=axes[1], title='Trend')\n",
        "            ts_results['seasonal'].plot(ax=axes[2], title='Seasonal')\n",
        "            ts_results['residual'].plot(ax=axes[3], title='Residual')\n",
        "            plt.tight_layout()\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "EwOap9eEmbWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# CELL 8: Customer Segmentation Analysis\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üë• CUSTOMER SEGMENTATION ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize customer segmenter\n",
        "segmenter = CustomerSegmenter()\n",
        "\n",
        "# Prepare data for segmentation\n",
        "segmentation_features = ['total_spent', 'avg_order_value', 'order_frequency',\n",
        "                        'customer_lifetime_days', 'satisfaction_score']\n",
        "\n",
        "# Check which features are available\n",
        "available_features = [f for f in segmentation_features if f in df.columns]\n",
        "print(f\"üìä Using features for segmentation: {available_features}\")\n",
        "\n",
        "if len(available_features) >= 3:\n",
        "    # Perform RFM analysis\n",
        "    print(\"\\nüîÑ Performing RFM Analysis...\")\n",
        "\n",
        "    # Create RFM-like features if not available\n",
        "    if 'recency_days' not in df.columns:\n",
        "        df['recency_days'] = np.random.randint(1, 365, len(df))\n",
        "    if 'total_spent' not in df.columns:\n",
        "        df['total_spent'] = df['total_amount'] * np.random.uniform(1, 5, len(df))\n",
        "    if 'order_frequency' not in df.columns:\n",
        "        df['order_frequency'] = np.random.randint(1, 20, len(df))\n",
        "\n",
        "    rfm_segments = segmenter.rfm_segmentation(df)\n",
        "\n",
        "    print(\"‚úÖ RFM Segmentation completed!\")\n",
        "    print(f\"üìä Segment distribution:\")\n",
        "    print(rfm_segments['segments'].value_counts())\n",
        "\n",
        "    # Perform K-means clustering\n",
        "    print(\"\\nüîÑ Performing K-means clustering...\")\n",
        "\n",
        "    clustering_data = df[available_features].fillna(0)\n",
        "    kmeans_result = segmenter.kmeans_segmentation(clustering_data, n_clusters=5)\n",
        "\n",
        "    print(\"‚úÖ K-means clustering completed!\")\n",
        "    print(f\"üìä Cluster distribution:\")\n",
        "    cluster_counts = pd.Series(kmeans_result['labels']).value_counts().sort_index()\n",
        "    print(cluster_counts)\n",
        "\n",
        "    # Add cluster labels to dataframe\n",
        "    df['cluster'] = kmeans_result['labels']\n",
        "    df['rfm_segment'] = rfm_segments['segments']\n",
        "\n",
        "    # Visualize clusters\n",
        "    if len(available_features) >= 2:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        scatter = plt.scatter(df[available_features[0]],\n",
        "                            df[available_features[1]],\n",
        "                            c=df['cluster'], cmap='viridis', alpha=0.6)\n",
        "        plt.xlabel(available_features[0])\n",
        "        plt.ylabel(available_features[1])\n",
        "        plt.title('K-means Clustering')\n",
        "        plt.colorbar(scatter)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        rfm_numeric = pd.Categorical(df['rfm_segment']).codes\n",
        "        scatter2 = plt.scatter(df[available_features[0]],\n",
        "                             df[available_features[1]],\n",
        "                             c=rfm_numeric, cmap='Set3', alpha=0.6)\n",
        "        plt.xlabel(available_features[0])\n",
        "        plt.ylabel(available_features[1])\n",
        "        plt.title('RFM Segmentation')\n",
        "        plt.colorbar(scatter2)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 9: Machine Learning Model Training - Regression\n",
        "# ============================================================================\n",
        "\n",
        "print(\"ü§ñ MACHINE LEARNING MODEL TRAINING - REGRESSION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize ML optimizer\n",
        "ml_optimizer = MachineLearningOptimizer()\n",
        "\n",
        "# Train regression models for each numerical target\n",
        "regression_targets = ['total_amount', 'profit', 'satisfaction_score', 'customer_lifetime_days']\n",
        "\n",
        "regression_results = {}\n",
        "\n",
        "for target in regression_targets:\n",
        "    if target in df.columns:\n",
        "        print(f\"\\nüéØ Training models for target: {target}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Prepare data\n",
        "        X_train, X_test, y_train, y_test = ml_optimizer.prepare_data(\n",
        "            df, target, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"üìä Train set shape: {X_train.shape}\")\n",
        "        print(f\"üìä Test set shape: {X_test.shape}\")\n",
        "\n",
        "        # Train regression models\n",
        "        results = ml_optimizer.train_regression_models(X_train, y_train, X_test, y_test)\n",
        "        regression_results[target] = results\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\nüìà Model Performance for {target}:\")\n",
        "        for model_name, metrics in results.items():\n",
        "            if 'error' not in metrics:\n",
        "                print(f\"  {model_name}:\")\n",
        "                print(f\"    Test R¬≤: {metrics['test_r2']:.4f}\")\n",
        "                print(f\"    Test MSE: {metrics['test_mse']:.4f}\")\n",
        "\n",
        "        # Find best model\n",
        "        best_model = max(results.items(),\n",
        "                        key=lambda x: x[1].get('test_r2', -np.inf) if 'error' not in x[1] else -np.inf)\n",
        "        print(f\"\\nüèÜ Best model for {target}: {best_model[0]} (R¬≤ = {best_model[1]['test_r2']:.4f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 10: Machine Learning Model Training - Classification\n",
        "# ============================================================================\n",
        "\n",
        "print(\"ü§ñ MACHINE LEARNING MODEL TRAINING - CLASSIFICATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train classification models for binary targets\n",
        "classification_targets = ['is_returned']\n",
        "\n",
        "classification_results = {}\n",
        "\n",
        "for target in classification_targets:\n",
        "    if target in df.columns:\n",
        "        print(f\"\\nüéØ Training models for target: {target}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Prepare data\n",
        "        X_train, X_test, y_train, y_test = ml_optimizer.prepare_data(\n",
        "            df, target, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"üìä Train set shape: {X_train.shape}\")\n",
        "        print(f\"üìä Test set shape: {X_test.shape}\")\n",
        "        print(f\"üìä Class distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
        "\n",
        "        # Train classification models\n",
        "        results = ml_optimizer.train_classification_models(X_train, y_train, X_test, y_test)\n",
        "        classification_results[target] = results\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\nüìà Model Performance for {target}:\")\n",
        "        for model_name, metrics in results.items():\n",
        "            if 'error' not in metrics:\n",
        "                print(f\"  {model_name}:\")\n",
        "                print(f\"    Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
        "                print(f\"    F1 Score: {metrics['f1_score']:.4f}\")\n",
        "                if 'roc_auc' in metrics:\n",
        "                    print(f\"    ROC AUC: {metrics['roc_auc']:.4f}\")\n",
        "\n",
        "        # Find best model\n",
        "        best_model = max(results.items(),\n",
        "                        key=lambda x: x[1].get('test_accuracy', -np.inf) if 'error' not in x[1] else -np.inf)\n",
        "        print(f\"\\nüèÜ Best model for {target}: {best_model[0]} (Accuracy = {best_model[1]['test_accuracy']:.4f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 11: Hyperparameter Optimization\n",
        "# ============================================================================\n",
        "\n",
        "print(\"‚öôÔ∏è HYPERPARAMETER OPTIMIZATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Perform hyperparameter optimization for best performing models\n",
        "optimization_results = {}\n",
        "\n",
        "# Optimize for one regression target\n",
        "if 'total_amount' in df.columns:\n",
        "    print(\"üîÑ Optimizing Random Forest for total_amount prediction...\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = ml_optimizer.prepare_data(\n",
        "        df, 'total_amount', test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    opt_result = ml_optimizer.hyperparameter_optimization(\n",
        "        X_train, y_train, model_type='random_forest', task='regression'\n",
        "    )\n",
        "\n",
        "    optimization_results['total_amount_rf'] = opt_result\n",
        "\n",
        "    print(\"‚úÖ Optimization completed!\")\n",
        "    print(f\"üèÜ Best parameters: {opt_result['best_params']}\")\n",
        "    print(f\"üìä Best score: {opt_result['best_score']:.4f}\")\n",
        "\n",
        "# Optimize for classification target\n",
        "if 'is_returned' in df.columns:\n",
        "    print(\"\\nüîÑ Optimizing Random Forest for return prediction...\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = ml_optimizer.prepare_data(\n",
        "        df, 'is_returned', test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    opt_result = ml_optimizer.hyperparameter_optimization(\n",
        "        X_train, y_train, model_type='random_forest', task='classification'\n",
        "    )\n",
        "\n",
        "    optimization_results['is_returned_rf'] = opt_result\n",
        "\n",
        "    print(\"‚úÖ Optimization completed!\")\n",
        "    print(f\"üèÜ Best parameters: {opt_result['best_params']}\")\n",
        "    print(f\"üìä Best score: {opt_result['best_score']:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 12: Feature Importance Analysis\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üîç FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Analyze feature importance from trained models\n",
        "if hasattr(ml_optimizer, 'feature_importance') and ml_optimizer.feature_importance:\n",
        "\n",
        "    for model_name, importance_dict in ml_optimizer.feature_importance.items():\n",
        "        print(f\"\\nüéØ Feature Importance - {model_name}:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Sort features by importance\n",
        "        sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Display top 10 features\n",
        "        print(\"Top 10 most important features:\")\n",
        "        for i, (feature, importance) in enumerate(sorted_features[:10], 1):\n",
        "            print(f\"  {i:2d}. {feature:<30} {importance:.4f}\")\n",
        "\n",
        "        # Plot feature importance\n",
        "        if len(sorted_features) > 0:\n",
        "            features, importances = zip(*sorted_features[:15])\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            y_pos = np.arange(len(features))\n",
        "            plt.barh(y_pos, importances)\n",
        "            plt.yticks(y_pos, features)\n",
        "            plt.xlabel('Feature Importance')\n",
        "            plt.title(f'Top 15 Feature Importances - {model_name}')\n",
        "            plt.gca().invert_yaxis()\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# Perform additional feature selection\n",
        "print(\"\\nüîç ADDITIONAL FEATURE SELECTION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if 'total_amount' in df.columns:\n",
        "    X_train, X_test, y_train, y_test = ml_optimizer.prepare_data(\n",
        "        df, 'total_amount', test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Mutual information feature selection\n",
        "    selected_features = ml_optimizer.feature_selection(\n",
        "        X_train, y_train, method='mutual_info', k=20\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Selected {selected_features.shape[1]} features using mutual information\")\n",
        "    print(f\"üìä Selected features: {list(selected_features.columns)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 13: Deep Learning Models (if TensorFlow available)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üß† DEEP LEARNING MODELS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check if TensorFlow is available\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from utils import DeepLearningOptimizer\n",
        "\n",
        "    print(\"‚úÖ TensorFlow available - Training deep learning models...\")\n",
        "\n",
        "    # Initialize deep learning optimizer\n",
        "    dl_optimizer = DeepLearningOptimizer()\n",
        "\n",
        "    # Train neural network for regression\n",
        "    if 'total_amount' in df.columns:\n",
        "        print(\"\\nüîÑ Training neural network for total_amount prediction...\")\n",
        "\n",
        "        X_train, X_test, y_train, y_test = ml_optimizer.prepare_data(\n",
        "            df, 'total_amount', test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Scale features for neural network\n",
        "        X_train_scaled, X_test_scaled = ml_optimizer.scale_features(\n",
        "            X_train, X_test, method='standard'\n",
        "        )\n",
        "\n",
        "        # Train neural network\n",
        "        nn_results = dl_optimizer.train_neural_network(\n",
        "            X_train_scaled, y_train, X_test_scaled, y_test,\n",
        "            task='regression', epochs=50, batch_size=32\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Neural network training completed!\")\n",
        "        print(f\"üìä Final test MSE: {nn_results['test_mse']:.4f}\")\n",
        "        print(f\"üìä Final test R¬≤: {nn_results['test_r2']:.4f}\")\n",
        "\n",
        "        # Plot training history\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(nn_results['history']['loss'], label='Training Loss')\n",
        "        plt.plot(nn_results['history']['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        if 'mae' in nn_results['history']:\n",
        "            plt.plot(nn_results['history']['mae'], label='Training MAE')\n",
        "            plt.plot(nn_results['history']['val_mae'], label='Validation MAE')\n",
        "            plt.title('Model MAE')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('MAE')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "except ImportError:\n",
        "    print(\"‚ùå TensorFlow not available - Skipping deep learning models\")\n",
        "    print(\"üí° Install TensorFlow to enable deep learning features: pip install tensorflow\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 14: Sales Forecasting\n",
        "# ============================================================================\n",
        "\n",
        "print(\"üìà SALES FORECASTING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize sales forecaster\n",
        "forecaster = SalesForecaster()\n",
        "\n",
        "# Prepare time series data for forecasting\n",
        "if 'date' in df.columns and 'total_amount' in df.columns:\n",
        "\n",
        "    # Aggregate sales by date\n",
        "    daily_sales = df.groupby('date')['total_amount'].sum().sort_index()\n",
        "\n",
        "    print(f\"üìä Time series data shape: {daily_sales.shape}\")\n",
        "    print(f\"üìä Date range: {daily_sales.index.min()} to {daily_sales.index.max()}\")\n",
        "\n",
        "    # Simple moving average forecast\n",
        "    print(\"\\nüîÑ Generating moving average forecast...\")\n",
        "    ma_forecast = forecaster.moving_average_forecast(daily_sales, window=3, periods=12)\n",
        "\n",
        "    print(\"‚úÖ Moving average forecast generated!\")\n",
        "    print(f\"üìä Forecast for next 12 periods:\")\n",
        "    print(ma_forecast.round(2))\n",
        "\n",
        "    # Exponential smoothing forecast\n",
        "    print(\"\\nüîÑ Generating exponential smoothing forecast...\")\n",
        "    es_forecast = forecaster.exponential_smoothing_forecast(daily_sales, periods=12)\n",
        "\n",
        "    print(\"‚úÖ Exponential smoothing forecast generated!\")\n",
        "    print(f\"üìä Forecast for next 12 periods:\")\n",
        "    print(es_forecast.round(2))\n",
        "\n",
        "    # Plot forecasts\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Plot historical data\n",
        "    plt.plot(daily_sales.index, daily_sales.values, label='Historical Sales', color='blue')\n",
        "\n",
        "    # Create future dates for plotting\n",
        "    last_date = daily_sales.index[-1]\n",
        "    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=12, freq='M')\n",
        "\n",
        "    # Plot forecasts\n",
        "    plt.plot(future_dates, ma_forecast, label='Moving Average Forecast',\n",
        "             color='red', linestyle='--', marker='o')\n",
        "    plt.plot(future_dates, es_forecast, label='Exponential Smoothing Forecast',\n",
        "             color='green', linestyle='--', marker='s')\n",
        "\n",
        "    plt.title('"
      ],
      "metadata": {
        "id": "y_ZTIkUW0574"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SECTION 4: DATA EXPLORATION & PREPROCESSING\n",
        "# ============================================================\n",
        "\n",
        "# Basic data info\n",
        "print(\"üìä DATASET OVERVIEW\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"\\nMissing values:\")\n",
        "print(df.isnull().sum().sum())\n",
        "\n",
        "# Display basic statistics\n",
        "print(f\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "# Handle missing values\n",
        "print(\"\\nüîß HANDLING MISSING VALUES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Fill numerical columns with median\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "for col in numerical_cols:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        df[col].fillna(df[col].median(), inplace=True)\n",
        "        print(f\"Filled {col} missing values with median\")\n",
        "\n",
        "# Fill categorical columns with mode\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "        print(f\"Filled {col} missing values with mode\")\n",
        "\n",
        "print(\"‚úÖ Missing values handled\")\n",
        "\n",
        "# Additional feature engineering\n",
        "print(\"\\n‚öôÔ∏è FEATURE ENGINEERING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create additional features if they don't exist\n",
        "if 'revenue_per_customer' not in df.columns:\n",
        "    df['revenue_per_customer'] = df.get('total_amount', 0) / (df.get('order_frequency', 1) + 1e-8)\n",
        "    print(\"‚úÖ Created revenue_per_customer\")\n",
        "\n",
        "if 'profit_per_unit' not in df.columns:\n",
        "    df['profit_per_unit'] = df.get('profit', 0) / (df.get('quantity', 1) + 1e-8)\n",
        "    print(\"‚úÖ Created profit_per_unit\")\n",
        "\n",
        "# Separate features and targets\n",
        "X = df[feature_columns].copy()\n",
        "y = df[list(targets.keys())].copy()\n",
        "\n",
        "print(f\"\\nüìà FINAL DATA SHAPE\")\n",
        "print(f\"Features: {X.shape}\")\n",
        "print(f\"Targets: {y.shape}\")\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 5: EXPLORATORY DATA ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nüîç EXPLORATORY DATA ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Target distributions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "fig.suptitle('Target Variables Distribution Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, (target, target_type) in enumerate(targets.items()):\n",
        "    row, col = i // 2, i % 2\n",
        "\n",
        "    if target_type == 'regression':\n",
        "        y[target].hist(bins=50, ax=axes[row, col], alpha=0.7, color=f'C{i}')\n",
        "        axes[row, col].set_title(f'{target.replace(\"_\", \" \").title()} Distribution')\n",
        "        axes[row, col].set_xlabel(target.replace(\"_\", \" \").title())\n",
        "        axes[row, col].set_ylabel('Frequency')\n",
        "\n",
        "        # Add statistics\n",
        "        mean_val = y[target].mean()\n",
        "        median_val = y[target].median()\n",
        "        axes[row, col].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
        "        axes[row, col].axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'Median: {median_val:.2f}')\n",
        "        axes[row, col].legend()\n",
        "    else:\n",
        "        value_counts = y[target].value_counts()\n",
        "        value_counts.plot(kind='bar', ax=axes[row, col], alpha=0.7, color=f'C{i}')\n",
        "        axes[row, col].set_title(f'{target.replace(\"_\", \" \").title()} Distribution')\n",
        "        axes[row, col].set_xlabel('Class')\n",
        "        axes[row, col].set_ylabel('Count')\n",
        "        axes[row, col].tick_params(axis='x', rotation=0)\n",
        "\n",
        "        # Add percentages\n",
        "        total = value_counts.sum()\n",
        "        for j, v in enumerate(value_counts.values):\n",
        "            axes[row, col].text(j, v + total*0.01, f'{v/total*100:.1f}%',\n",
        "                               ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis for top numerical features\n",
        "numerical_features = X.select_dtypes(include=[np.number]).columns[:25]  # Top 25 for readability\n",
        "\n",
        "if len(numerical_features) > 1:\n",
        "    print(f\"\\nüìä Correlation Analysis (Top {len(numerical_features)} Numerical Features)\")\n",
        "\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    correlation_matrix = X[numerical_features].corr()\n",
        "\n",
        "    # Create heatmap\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
        "               fmt='.2f', square=True, cbar_kws={\"shrink\": .8})\n",
        "    plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Find highly correlated features\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            corr_val = correlation_matrix.iloc[i, j]\n",
        "            if abs(corr_val) > 0.8:\n",
        "                high_corr_pairs.append((correlation_matrix.columns[i],\n",
        "                                      correlation_matrix.columns[j],\n",
        "                                      corr_val))\n",
        "\n",
        "    if high_corr_pairs:\n",
        "        print(f\"\\n‚ö†Ô∏è Highly correlated feature pairs (|r| > 0.8):\")\n",
        "        for feat1, feat2, corr in high_corr_pairs:\n",
        "            print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 6: FEATURE IMPORTANCE ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nüìà FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Analyze feature importance for each target\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "fig.suptitle('Feature Importance Analysis by Target', fontsize=16, fontweight='bold')\n",
        "\n",
        "feature_importance_results = {}\n",
        "\n",
        "for i, (target, target_type) in enumerate(targets.items()):\n",
        "    row, col = i // 2, i % 2\n",
        "\n",
        "    print(f\"Analyzing feature importance for {target}...\")\n",
        "\n",
        "    # Handle missing values in target\n",
        "    mask = ~y[target].isnull()\n",
        "    X_clean = X[mask]\n",
        "    y_clean = y[target][mask]\n",
        "\n",
        "    # Train a Random Forest for feature importance\n",
        "    if target_type == 'regression':\n",
        "        model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    else:\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "\n",
        "    try:\n",
        "        model.fit(X_clean, y_clean)\n",
        "\n",
        "        # Get feature importance\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X_clean.columns,\n",
        "            'importance': model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        # Store results\n",
        "        feature_importance_results[target] = feature_importance\n",
        "\n",
        "        # Plot top 15 features\n",
        "        top_features = feature_importance.head(15)\n",
        "        sns.barplot(data=top_features, y='feature', x='importance', ax=axes[row, col],\n",
        "                   palette='viridis')\n",
        "        axes[row, col].set_title(f'Top 15 Features for {target.replace(\"_\", \" \").title()}')\n",
        "        axes[row, col].set_xlabel('Importance Score')\n",
        "\n",
        "        print(f\"  ‚úÖ Top 5 features: {', '.join(top_features['feature'].head(5).tolist())}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error analyzing {target}: {str(e)}\")\n",
        "        axes[row, col].text(0.5, 0.5, f'Error analyzing\\n{target}',\n",
        "                           ha='center', va='center', transform=axes[row, col].transAxes)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 7: DATA PREPARATION FOR TRAINING\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nüîß PREPARING DATA FOR TRAINING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Store all data splits and preprocessing objects\n",
        "data_splits = {}\n",
        "scalers = {}\n",
        "feature_selectors = {}\n",
        "\n",
        "for target, target_type in targets.items():\n",
        "    print(f\"\\nPreparing data for {target}...\")\n",
        "\n",
        "    # Handle missing values in target\n",
        "    mask = ~y[target].isnull()\n",
        "    X_target = X[mask].copy()\n",
        "    y_target = y[target][mask].copy()\n",
        "\n",
        "    print(f\"  Available samples: {len(X_target)}\")\n",
        "\n",
        "    # Split the data\n",
        "    if target_type == 'classification' and len(y_target.unique()) > 1:\n",
        "        # Stratified split for classification\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_target, y_target, test_size=0.2, random_state=RANDOM_STATE,\n",
        "            stratify=y_target\n",
        "        )\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_target, y_target, test_size=0.2, random_state=RANDOM_STATE\n",
        "        )\n",
        "\n",
        "    print(f\"  Train set: {X_train.shape}\")\n",
        "    print(f\"  Test set: {X_test.shape}\")\n",
        "\n",
        "    # Scale features using RobustScaler (less sensitive to outliers)\n",
        "    scaler = RobustScaler()\n",
        "    X_train_scaled = pd.DataFrame(\n",
        "        scaler.fit_transform(X_train),\n",
        "        columns=X_train.columns,\n",
        "        index=X_train.index\n",
        "    )\n",
        "    X_test_scaled = pd.DataFrame(\n",
        "        scaler.transform(X_test),\n",
        "        columns=X_test.columns,\n",
        "        index=X_test.index\n",
        "    )\n",
        "\n",
        "    scalers[target] = scaler\n",
        "\n",
        "    # Feature selection - select top K features\n",
        "    k_features = min(50, X_train.shape[1])  # Select top 50 features or all if less\n",
        "\n",
        "    if target_type == 'regression':\n",
        "        selector = SelectKBest(f_regression, k=k_features)\n",
        "    else:\n",
        "        selector = SelectKBest(f_classif, k=k_features)\n",
        "\n",
        "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "    X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "    feature_selectors[target] = selector\n",
        "    selected_features = X_train_scaled.columns[selector.get_support()].tolist()\n",
        "\n",
        "    print(f\"  Selected {k_features} features\")\n",
        "    print(f\"  Top 5 selected: {selected_features[:5]}\")\n",
        "\n",
        "    # Store everything\n",
        "    data_splits[target] = {\n",
        "        'X_train': X_train_selected,\n",
        "        'X_test': X_test_selected,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test,\n",
        "        'feature_names': selected_features,\n",
        "        'original_X_train': X_train,\n",
        "        'original_X_test': X_test\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Data preparation completed!\")\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 8: TRADITIONAL MACHINE LEARNING MODELS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nü§ñ TRAINING TRADITIONAL MACHINE LEARNING MODELS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Store all models and performance metrics\n",
        "models = {}\n",
        "performance_metrics = {}\n",
        "\n",
        "for target, target_type in targets.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TRAINING MODELS FOR {target.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Get data\n",
        "    X_train = data_splits[target]['X_train']\n",
        "    X_test = data_splits[target]['X_test']\n",
        "    y_train = data_splits[target]['y_train']\n",
        "    y_test = data_splits[target]['y_test']\n",
        "\n",
        "    print(f\"Training data shape: {X_train.shape}\")\n",
        "    print(f\"Target type: {target_type}\")\n",
        "\n",
        "    # Define models based on target type\n",
        "    if target_type == 'regression':\n",
        "        target_models = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'Ridge Regression': Ridge(alpha=1.0, random_state=RANDOM_STATE),\n",
        "            'Lasso Regression': Lasso(alpha=1.0, random_state=RANDOM_STATE, max_iter=2000),\n",
        "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
        "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=RANDOM_STATE),\n",
        "            'XGBoost': xgb.XGBRegressor(random_state=RANDOM_STATE, eval_metric='rmse', verbosity=0),\n",
        "            'LightGBM': lgb.LGBMRegressor(random_state=RANDOM_STATE, verbose=-1, force_col_wise=True)\n",
        "        }\n",
        "    else:\n",
        "        target_models = {\n",
        "            'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
        "            'XGBoost': xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss', verbosity=0),\n",
        "            'LightGBM': lgb.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1, force_col_wise=True)\n",
        "        }\n",
        "\n",
        "    # Train each model\n",
        "    target_models_trained = {}\n",
        "    target_metrics = {}\n",
        "\n",
        "    for model_name, model in target_models.items():\n",
        "        print(f\"\\nüîÑ Training {model_name}...\")\n",
        "\n",
        "        try:\n",
        "            # Train the model\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            if target_type == 'regression':\n",
        "                mse = mean_squared_error(y_test, y_pred)\n",
        "                rmse = np.sqrt(mse)\n",
        "                mae = mean_absolute_error(y_test, y_pred)\n",
        "                r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "                metrics = {\n",
        "                    'MSE': mse,\n",
        "                    'RMSE': rmse,\n",
        "                    'MAE': mae,\n",
        "                    'R2': r2\n",
        "                }\n",
        "\n",
        "                print(f\"    R¬≤ Score: {r2:.4f}\")\n",
        "                print(f\"    RMSE: {rmse:.4f}\")\n",
        "                print(f\"    MAE: {mae:.4f}\")\n",
        "\n",
        "            else:\n",
        "                # Classification metrics\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "                # AUC for binary classification\n",
        "                auc = 0\n",
        "                if hasattr(model, 'predict_proba') and len(np.unique(y_test)) == 2:\n",
        "                    try:\n",
        "                        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "                        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                metrics = {\n",
        "                    'Accuracy': accuracy,\n",
        "                    'Precision': precision,\n",
        "                    'Recall': recall,\n",
        "                    'F1': f1,\n",
        "                    'AUC': auc\n",
        "                }\n",
        "\n",
        "                print(f\"    Accuracy: {accuracy:.4f}\")\n",
        "                print(f\"    F1 Score: {f1:.4f}\")\n",
        "                if auc > 0:\n",
        "                    print(f\"    AUC: {auc:.4f}\")\n",
        "\n",
        "            target_models_trained[model_name] = model\n",
        "            target_metrics[model_name] = metrics\n",
        "            print(f\"    ‚úÖ {model_name} completed successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ùå Error training {model_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Store results\n",
        "    models[target] = target_models_trained\n",
        "    performance_metrics[target] = target_metrics\n",
        "\n",
        "print(\"\\n‚úÖ Traditional ML models training completed!\")\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 9: DEEP LEARNING MODELS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nüß† TRAINING DEEP LEARNING MODELS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for target, target_type in targets.items():\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"Building DL model for {target.upper()}\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    # Get data\n",
        "    X_train = data_splits[target]['X_train']\n",
        "    X_test = data_splits[target]['X_test']\n",
        "    y_train = data_splits[target]['y_train']\n",
        "    y_test = data_splits[target]['y_test']\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    print(f\"Input dimension: {input_dim}\")\n",
        "    print(f\"Training samples: {X_train.shape[0]}\")\n",
        "\n",
        "    # Build model architecture\n",
        "    model = keras.Sequential([\n",
        "        # Input layer with dropout\n",
        "        layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # Hidden layers\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "    ])\n",
        "\n",
        "    # Output layer and compilation\n",
        "    if target_type == 'regression':\n",
        "        model.add(layers.Dense(1, activation='linear'))\n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='mse',\n",
        "            metrics=['mae', 'mse']\n",
        "        )\n",
        "        monitor_metric = 'val_loss'\n",
        "    else:\n",
        "        model.add(layers.Dense(1, activation='sigmoid'))\n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        monitor_metric = 'val_loss'\n",
        "\n",
        "    print(f\"Model architecture:\")\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = callbacks.EarlyStopping(\n",
        "        monitor=monitor_metric,\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "        monitor=monitor_metric,\n",
        "        factor=0.5,\n",
        "        patience=10,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"\\nüîÑ Training Deep Learning model...\")\n",
        "\n",
        "    try:\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Plot training history\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title(f'Model Loss - {target}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        if target_type == 'regression':\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(history.history['mae'], label='Training MAE')\n",
        "            plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "            plt.title(f'Model MAE - {target}')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('MAE')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "        else:\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "            plt.title(f'Model Accuracy - {target}')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "        # Calculate metrics\n",
        "        if target_type == 'regression':\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            rmse = np.sqrt(mse)\n",
        "            mae = mean_absolute_error(y_"
      ],
      "metadata": {
        "id": "M1qmz8geOpeK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}